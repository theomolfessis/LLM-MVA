{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import DataLoaderECHR\n",
    "data_path= \"/users/eleves-b/2021/andrei.barbu/ECHR_Dataset/\"\n",
    "df_dict  = DataLoaderECHR(data_path + \"EN_train\", data_path + \"EN_dev\", data_path + \"EN_test\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_dict['train']\n",
    "df_train['n_sentences'] = df_train['TEXT'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train['n_sentences'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ITEMID</th>\n",
       "      <th>LANGUAGEISOCODE</th>\n",
       "      <th>RESPONDENT</th>\n",
       "      <th>BRANCH</th>\n",
       "      <th>DATE</th>\n",
       "      <th>DOCNAME</th>\n",
       "      <th>IMPORTANCE</th>\n",
       "      <th>CONCLUSION</th>\n",
       "      <th>JUDGES</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>VIOLATED_ARTICLES</th>\n",
       "      <th>VIOLATED_PARAGRAPHS</th>\n",
       "      <th>VIOLATED_BULLETPOINTS</th>\n",
       "      <th>NON_VIOLATED_ARTICLES</th>\n",
       "      <th>NON_VIOLATED_PARAGRAPHS</th>\n",
       "      <th>NON_VIOLATED_BULLETPOINTS</th>\n",
       "      <th>VIOLATED</th>\n",
       "      <th>n_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001-100005</td>\n",
       "      <td>ENG</td>\n",
       "      <td>TUR</td>\n",
       "      <td>ADMISSIBILITY</td>\n",
       "      <td>2010</td>\n",
       "      <td>SHAMSI v. TURKEY</td>\n",
       "      <td>4</td>\n",
       "      <td>Inadmissible</td>\n",
       "      <td>Françoise Tulkens;Ireneu Cabral Barreto;Kristi...</td>\n",
       "      <td>[The applicant, Mr Maher Muhilddin Gazel Al Sh...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001-100024</td>\n",
       "      <td>ENG</td>\n",
       "      <td>ARM</td>\n",
       "      <td>CHAMBER</td>\n",
       "      <td>2010</td>\n",
       "      <td>CASE OF HOVHANNISYAN AND SHIROYAN v. ARMENIA</td>\n",
       "      <td>3</td>\n",
       "      <td>Reminder inadmissible;Violation of P1-1;Just s...</td>\n",
       "      <td>Alvina Gyulumyan;Elisabet Fura;Ineta Ziemele;J...</td>\n",
       "      <td>[4. The applicants were born in 1976, 1973 and...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001-100026</td>\n",
       "      <td>ENG</td>\n",
       "      <td>ARM</td>\n",
       "      <td>CHAMBER</td>\n",
       "      <td>2010</td>\n",
       "      <td>CASE OF YERANOSYAN AND OTHERS v. ARMENIA</td>\n",
       "      <td>4</td>\n",
       "      <td>Violation of P1-1</td>\n",
       "      <td>Alvina Gyulumyan;Elisabet Fura;Ineta Ziemele;J...</td>\n",
       "      <td>[4. The applicants were born in 1976, 1975, 19...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001-100029</td>\n",
       "      <td>ENG</td>\n",
       "      <td>RUS</td>\n",
       "      <td>CHAMBER</td>\n",
       "      <td>2010</td>\n",
       "      <td>CASE OF AKHMATKHANOVY v. RUSSIA</td>\n",
       "      <td>4</td>\n",
       "      <td>Violation of Art. 2 (substantive aspect);Viola...</td>\n",
       "      <td>Anatoly Kovler;Christos Rozakis;Dean Spielmann...</td>\n",
       "      <td>[4. The applicants are:, 1) Ms Bilat Akhmatkha...</td>\n",
       "      <td>[13, 2, 3, 5]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001-100038</td>\n",
       "      <td>ENG</td>\n",
       "      <td>NLD</td>\n",
       "      <td>CHAMBER</td>\n",
       "      <td>2010</td>\n",
       "      <td>CASE OF A. v. THE NETHERLANDS</td>\n",
       "      <td>3</td>\n",
       "      <td>Violation of Art. 3 (in case of expulsion to L...</td>\n",
       "      <td>Alvina Gyulumyan;Corneliu Bîrsan;Egbert Myjer;...</td>\n",
       "      <td>[7. The applicant was born in 1972 and lives i...</td>\n",
       "      <td>[3]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[13]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ITEMID LANGUAGEISOCODE RESPONDENT         BRANCH  DATE  \\\n",
       "0  001-100005             ENG        TUR  ADMISSIBILITY  2010   \n",
       "1  001-100024             ENG        ARM        CHAMBER  2010   \n",
       "2  001-100026             ENG        ARM        CHAMBER  2010   \n",
       "3  001-100029             ENG        RUS        CHAMBER  2010   \n",
       "4  001-100038             ENG        NLD        CHAMBER  2010   \n",
       "\n",
       "                                        DOCNAME IMPORTANCE  \\\n",
       "0                              SHAMSI v. TURKEY          4   \n",
       "1  CASE OF HOVHANNISYAN AND SHIROYAN v. ARMENIA          3   \n",
       "2      CASE OF YERANOSYAN AND OTHERS v. ARMENIA          4   \n",
       "3               CASE OF AKHMATKHANOVY v. RUSSIA          4   \n",
       "4                 CASE OF A. v. THE NETHERLANDS          3   \n",
       "\n",
       "                                          CONCLUSION  \\\n",
       "0                                       Inadmissible   \n",
       "1  Reminder inadmissible;Violation of P1-1;Just s...   \n",
       "2                                  Violation of P1-1   \n",
       "3  Violation of Art. 2 (substantive aspect);Viola...   \n",
       "4  Violation of Art. 3 (in case of expulsion to L...   \n",
       "\n",
       "                                              JUDGES  \\\n",
       "0  Françoise Tulkens;Ireneu Cabral Barreto;Kristi...   \n",
       "1  Alvina Gyulumyan;Elisabet Fura;Ineta Ziemele;J...   \n",
       "2  Alvina Gyulumyan;Elisabet Fura;Ineta Ziemele;J...   \n",
       "3  Anatoly Kovler;Christos Rozakis;Dean Spielmann...   \n",
       "4  Alvina Gyulumyan;Corneliu Bîrsan;Egbert Myjer;...   \n",
       "\n",
       "                                                TEXT VIOLATED_ARTICLES  \\\n",
       "0  [The applicant, Mr Maher Muhilddin Gazel Al Sh...                []   \n",
       "1  [4. The applicants were born in 1976, 1973 and...                []   \n",
       "2  [4. The applicants were born in 1976, 1975, 19...                []   \n",
       "3  [4. The applicants are:, 1) Ms Bilat Akhmatkha...     [13, 2, 3, 5]   \n",
       "4  [7. The applicant was born in 1972 and lives i...               [3]   \n",
       "\n",
       "  VIOLATED_PARAGRAPHS VIOLATED_BULLETPOINTS NON_VIOLATED_ARTICLES  \\\n",
       "0                  []                    []                    []   \n",
       "1                  []                    []                    []   \n",
       "2                  []                    []                    []   \n",
       "3                  []                    []                    []   \n",
       "4                  []                    []                  [13]   \n",
       "\n",
       "  NON_VIOLATED_PARAGRAPHS NON_VIOLATED_BULLETPOINTS  VIOLATED  n_sentences  \n",
       "0                      []                        []         0            8  \n",
       "1                      []                        []         0           25  \n",
       "2                      []                        []         0           15  \n",
       "3                      []                        []         1          123  \n",
       "4                      []                        []         1          224  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict[\"train\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-b/2021/andrei.barbu/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from transformers import AutoModel\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 freeze_bert=False, \n",
    "                 use_lora=False, \n",
    "                 lora_rank=4,\n",
    "                 use_attention=False,\n",
    "                 max_sentences=128,\n",
    "                 dropout_rate=0.1,\n",
    "                 modelname   = \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "         - freeze_bert: If True, freeze BERT parameters (unless using LoRA).\n",
    "         - use_lora: If True, apply a LoRA adapter to BERT's linear layers.\n",
    "         - lora_rank: The rank for low-rank adaptation in LoRA.\n",
    "         - use_attention: If True, use a hierarchical attention head over sentence embeddings.\n",
    "                          Otherwise, use an MLP head that concatenates sentence embeddings.\n",
    "         - max_sentences: Fixed number of sentences per document (with padding/truncation).\n",
    "         - dropout_rate: Dropout rate in the classification head.\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Load the pre-trained BERT model.\n",
    "        self.bert = AutoModel.from_pretrained(modelname)\n",
    "        print(\"Loaded BERT model:\", modelname)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        self.use_attention = use_attention\n",
    "        self.max_sentences = max_sentences\n",
    "\n",
    "        # Apply LoRA modifications if requested.\n",
    "        if use_lora:\n",
    "            apply_lora(self.bert, lora_rank)\n",
    "        elif freeze_bert:\n",
    "            # Freeze all BERT parameters if not using LoRA.\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        if use_attention:\n",
    "            # For the hierarchical attention mechanism, we compute explicit query, key, and value.\n",
    "            # The keys and values are computed from each sentence embedding.\n",
    "            self.key_layer = nn.Linear(self.hidden_size, self.hidden_size)    # Projection for keys\n",
    "            self.value_layer = nn.Linear(self.hidden_size, self.hidden_size)  # Projection for values\n",
    "            # A learned context vector (used as the query) that is shared across sentences.\n",
    "            # This is analogous to a fixed query vector in attention mechanisms.\n",
    "            self.context_vector = nn.Parameter(torch.randn(self.hidden_size))\n",
    "            # After attention, we pass the aggregated representation through a dense layer.\n",
    "            self.dense_tanh = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "            self.classifier = nn.Linear(self.hidden_size, 2)\n",
    "        else:\n",
    "            # MLP-based head: concatenate sentence embeddings into a single vector.\n",
    "            self.dense_tanh = nn.Linear(max_sentences * self.hidden_size, max_sentences * self.hidden_size)\n",
    "            self.classifier = nn.Linear(max_sentences * self.hidden_size, 2)\n",
    "            \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for processing a batch of documents.\n",
    "        Each document is represented as a list of sentences.\n",
    "        \n",
    "        Parameters:\n",
    "         - input_ids: Tensor of shape (batch_size, max_sentences, seq_length)\n",
    "         - attention_mask: Tensor of shape (batch_size, max_sentences, seq_length)\n",
    "        \"\"\"\n",
    "        batch_size, num_sentences, seq_length = input_ids.size()\n",
    "        # Flatten the sentences so BERT processes each sentence independently.\n",
    "        input_ids_flat = input_ids.view(-1, seq_length)\n",
    "        attention_mask_flat = attention_mask.view(-1, seq_length)\n",
    "        \n",
    "        # Obtain BERT sentence embeddings (using the pooled [CLS] token representation).\n",
    "        outputs = self.bert(input_ids=input_ids_flat, attention_mask=attention_mask_flat)\n",
    "        pooled_output = outputs.pooler_output if hasattr(outputs, 'pooler_output') else outputs[1]\n",
    "        # Reshape back to (batch_size, num_sentences, hidden_size).\n",
    "        sentence_embeddings = pooled_output.view(batch_size, num_sentences, self.hidden_size)\n",
    "        \n",
    "        if self.use_attention:\n",
    "            # ----------------------\n",
    "            # Hierarchical Attention:\n",
    "            # ----------------------\n",
    "            # 1. Compute Keys (K) from sentence embeddings.\n",
    "            #    Each sentence embedding is projected to obtain its key representation.\n",
    "            K = self.key_layer(sentence_embeddings)  # shape: (batch_size, num_sentences, hidden_size)\n",
    "            \n",
    "            # 2. Compute Values (V) from sentence embeddings.\n",
    "            #    Here, the values are also derived from the sentence embeddings.\n",
    "            V = self.value_layer(sentence_embeddings)  # shape: (batch_size, num_sentences, hidden_size)\n",
    "            \n",
    "            # 3. Use a learned context vector as the Query (Q).\n",
    "            #    Expand the context vector to match the batch size and add a sequence length dimension.\n",
    "            #    Q shape: (batch_size, 1, hidden_size)\n",
    "            Q = self.context_vector.unsqueeze(0).expand(batch_size, -1).unsqueeze(1)\n",
    "            \n",
    "            # 4. Compute scaled dot-product attention scores:\n",
    "            #    scores = (Q * K^T) / sqrt(d_k)\n",
    "            #    Here, K^T means transposing the last two dimensions of K.\n",
    "            scores = torch.matmul(Q, K.transpose(-2, -1))  # shape: (batch_size, 1, num_sentences)\n",
    "            scores = scores / math.sqrt(self.hidden_size)     # scale by sqrt(d_k)\n",
    "            scores = scores.squeeze(1)                         # shape: (batch_size, num_sentences)\n",
    "            \n",
    "            # 5. Create a sentence mask to avoid attending to padded sentences.\n",
    "            #    A sentence is considered valid if its attention_mask has a nonzero sum.\n",
    "            sentence_mask = (attention_mask.sum(dim=2) > 0).float()  # shape: (batch_size, num_sentences)\n",
    "            # Set scores of padded sentences to a very low value.\n",
    "            scores = scores.masked_fill(sentence_mask == 0, -1e9)\n",
    "            \n",
    "            # 6. Apply softmax to obtain attention weights.\n",
    "            attn_weights = torch.softmax(scores, dim=1)          # shape: (batch_size, num_sentences)\n",
    "            attn_weights = attn_weights.unsqueeze(-1)            # shape: (batch_size, num_sentences, 1)\n",
    "            \n",
    "            # 7. Compute the context vector (document embedding) as the weighted sum of the values.\n",
    "            doc_embedding = torch.sum(V * attn_weights, dim=1)     # shape: (batch_size, hidden_size)\n",
    "            \n",
    "            # Pass the aggregated representation through a dense layer.\n",
    "            x = self.dropout(doc_embedding)\n",
    "            x = self.dense_tanh(x)\n",
    "        else:\n",
    "            # ---------------------------\n",
    "            # MLP-based head (non-attention):\n",
    "            # ---------------------------\n",
    "            # Concatenate sentence embeddings into a single vector.\n",
    "            doc_embedding = sentence_embeddings.view(batch_size, -1)  # shape: (batch_size, max_sentences*hidden_size)\n",
    "            x = self.dropout(doc_embedding)\n",
    "            x = self.dense_tanh(x)\n",
    "            \n",
    "        # Apply non-linearity and dropout.\n",
    "        x = self.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        # Final classification layer.\n",
    "        logits = self.classifier(x)\n",
    "        # Convert logits to probabilities.\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "\n",
    "# Helper function for applying LoRA to a module's linear layers.\n",
    "def apply_lora(module, lora_rank):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.Linear):\n",
    "            setattr(module, name, LoRALinear(child, lora_rank))\n",
    "        else:\n",
    "            apply_lora(child, lora_rank)\n",
    "\n",
    "# A simple LoRA wrapper for a linear layer.\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, linear_layer, lora_rank=4, scaling=1.0):\n",
    "        super(LoRALinear, self).__init__()\n",
    "        self.linear = linear_layer\n",
    "        self.lora_rank = lora_rank\n",
    "        self.scaling = scaling\n",
    "        # Create low-rank adaptation matrices.\n",
    "        self.lora_A = nn.Parameter(torch.zeros(lora_rank, linear_layer.in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(linear_layer.out_features, lora_rank))\n",
    "        # Initialize the low-rank matrices.\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "        # Freeze original linear layer parameters.\n",
    "        for param in self.linear.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        original = self.linear(x)\n",
    "        lora_update = self.scaling * ((x @ self.lora_A.t()) @ self.lora_B.t())\n",
    "        return original + lora_update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, text_column, label_column, max_sentences=10, max_seq_length=128,modelname=\"google/bert_uncased_L-2_H-128_A-2\"):\n",
    "        \"\"\"\n",
    "        Dataset that handles the TEXT feature where each entry is a list of sentences.\n",
    "        \n",
    "        Parameters:\n",
    "        - dataframe: Pandas DataFrame containing your data.\n",
    "        - text_column: Name of the column with the TEXT feature (a list of strings).\n",
    "        - label_column: Name of the target label column (binary label: 0 or 1).\n",
    "        - max_sentences: Maximum number of sentences per document (pad or truncate if needed).\n",
    "        - max_seq_length: Maximum sequence length (in tokens) per sentence.\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.text_column = text_column\n",
    "        self.label_column = label_column\n",
    "        self.max_sentences = max_sentences\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the list of sentences for this example.\n",
    "        texts = self.dataframe.iloc[idx][self.text_column]\n",
    "        label = self.dataframe.iloc[idx][self.label_column]\n",
    "        \n",
    "        # Ensure the input is a list. If not, wrap it into a list.\n",
    "        if not isinstance(texts, list):\n",
    "            texts = [texts]\n",
    "        \n",
    "        # Pad or truncate the list of sentences to max_sentences.\n",
    "        if len(texts) < self.max_sentences:\n",
    "            texts = texts + [\"\"] * (self.max_sentences - len(texts))\n",
    "        else:\n",
    "            texts = texts[:self.max_sentences]\n",
    "        \n",
    "        input_ids_list = []\n",
    "        attention_mask_list = []\n",
    "        \n",
    "        # Tokenize each sentence individually.\n",
    "        for sentence in texts:\n",
    "            encoding = self.tokenizer.encode_plus(\n",
    "                sentence,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_seq_length,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            input_ids_list.append(encoding['input_ids'])       # Tensor shape: (1, max_seq_length)\n",
    "            attention_mask_list.append(encoding['attention_mask'])  # Tensor shape: (1, max_seq_length)\n",
    "            \n",
    "        # Concatenate tokenized sentences to form a tensor of shape (max_sentences, max_seq_length)\n",
    "        input_ids = torch.cat(input_ids_list, dim=0)\n",
    "        attention_mask = torch.cat(attention_mask_list, dim=0)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,           # Shape: (max_sentences, max_seq_length)\n",
    "            'attention_mask': attention_mask, # Shape: (max_sentences, max_seq_length)\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the training and development data from your dictionary\n",
    "df_train = df_dict[\"train\"]\n",
    "\n",
    "df_train_dev = df_dict[\"train_dev\"]\n",
    "\n",
    "\n",
    "def run_experiments(param_dict, idxres, max_sentences=64, max_seq_length=256) : \n",
    "    # Create a file to store the results of the experiments.\n",
    "    max_sentences = max_sentences\n",
    "    max_seq_length = max_seq_length\n",
    "    with open(f\"results_{idxres}.txt\", \"w\") as f:\n",
    "        f.write(\"modelname,use_attention,use_lora,freeze_bert,num_epochs,batch_size,accuracy\\n\")\n",
    "        f.write(\"Evolution of loss during training\\n\")\n",
    "    \n",
    "    modelname = param_dict[\"modelname\"]\n",
    "    print(f\"Start experiment with model: {modelname}, use_attention: {param_dict['use_attention']}, use_lora: {param_dict['use_lora']}, freeze_bert: {param_dict['freeze_bert']}, num_epochs: {param_dict['num_epochs']}, batch_size: {param_dict['batch_size']}\")\n",
    "    # Initialize tokenizer for the BERT model.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
    "\n",
    "\n",
    "\n",
    "    # Create dataset and dataloader using the TEXT feature instead of CONCLUSION.\n",
    "    print(\"Creating dataset\")\n",
    "    dataset = TextDataset(\n",
    "        df_train_dev,\n",
    "        text_column='TEXT',\n",
    "        label_column='VIOLATED',\n",
    "        max_sentences=max_sentences,\n",
    "        max_seq_length=max_seq_length,\n",
    "        modelname=modelname\n",
    "    )\n",
    "    batch_size = param_dict[\"batch_size\"]\n",
    "    print(\"Loading dataset\")\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    use_attention = param_dict[\"use_attention\"]\n",
    "    use_lora = param_dict[\"use_lora\"]\n",
    "    lora_rank = param_dict[\"lora_rank\"]\n",
    "    freeze_bert = param_dict[\"freeze_bert\"]\n",
    "\n",
    "    # Initialize the model (ensure you have defined BertClassifier accordingly).\n",
    "    model = BertClassifier(dropout_rate=0.1, freeze_bert=freeze_bert, use_attention=use_attention, use_lora= use_lora, modelname=modelname,lora_rank=lora_rank, max_sentences=max_sentences)\n",
    "    print(\"Model initialized\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"Device:\", device)\n",
    "    model = model.to(device)\n",
    "    print(\"Model moved to device\")\n",
    "\n",
    "    # Set up training parameters.\n",
    "    learning_rate = 2e-5  # For grid-search, iterate over {2e-5, 3e-5, 4e-5, 5e-5}.\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    num_epochs = param_dict[\"num_epochs\"]\n",
    "    total_steps = len(dataloader) * num_epochs\n",
    "\n",
    "    # Scheduler with 100k warm-up steps.\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100000, num_training_steps=total_steps)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Early stopping parameters.\n",
    "    patience = 3\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    print(\"Start training\")\n",
    "    # Training loop.\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            # Move the inputs to the GPU if available.\n",
    "            # The input tensors have shape: (batch_size, max_sentences, max_seq_length)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass through the model.\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
    "        with open(f\"results_{idxres}.txt\", \"a\") as f:\n",
    "            f.write(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\\n\")\n",
    "        \n",
    "        # Early stopping check.\n",
    "        if avg_loss < best_loss - 5e-4 or epoch <10:\n",
    "            best_loss = avg_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save the best model checkpoint.\n",
    "            torch.save(model.state_dict(), f\"best_model_{idxres}.pt\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                with open(\"results.txt\", \"a\") as f:\n",
    "                    f.write(\"Early stopping triggered\")\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    def evaluate_only(param_dict, idxres):\n",
    "        modelname = param_dict[\"modelname\"]\n",
    "        model = BertClassifier(dropout_rate=0.1, freeze_bert=param_dict[\"freeze_bert\"], use_attention=param_dict[\"use_attention\"], use_lora= param_dict[\"use_lora\"], modelname=modelname,lora_rank=param_dict[\"lora_rank\"], max_sentences=max_sentences)\n",
    "        # Evaluation\n",
    "        # Assume df_test is your test DataFrame with the same \"CONCLUSION\" and \"VIOLATED\" columns.\n",
    "        df_test = df_dict[\"test\"]\n",
    "        test_dataset = TextDataset(df_test, text_column='TEXT', label_column='VIOLATED',modelname=modelname, max_sentences=max_sentences, max_seq_length=max_seq_length)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "        # Load the best model checkpoint\n",
    "\n",
    "        model.load_state_dict(torch.load(f\"best_model_{idxres}.pt\"))\n",
    "        model.eval()\n",
    "\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                # Get predicted labels from softmax probabilities\n",
    "                _, preds = torch.max(outputs, dim=1)\n",
    "                correct_predictions += (preds == labels).sum().item()\n",
    "                total_predictions += labels.size(0)\n",
    "                \n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        with open(f\"results_{idxres}.txt\", \"a\") as f:\n",
    "            f.write(f\"Test Accuracy: {accuracy:.4f}\\n\")\n",
    "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    evaluate_only(param_dict, idxres)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start experiment with model: google/bert_uncased_L-2_H-128_A-2, use_attention: False, use_lora: True, freeze_bert: False, num_epochs: 30, batch_size: 16\n",
      "Creating dataset\n",
      "Loading dataset\n",
      "Loaded BERT model: google/bert_uncased_L-2_H-128_A-2\n",
      "Model initialized\n",
      "Device: cuda\n",
      "Model moved to device\n",
      "Start training\n"
     ]
    }
   ],
   "source": [
    "param_dict = {\n",
    "    \"modelname\": \"google/bert_uncased_L-2_H-128_A-2\",\n",
    "    \"use_attention\": True,\n",
    "    \"use_lora\": False,\n",
    "    \"lora_rank\": 8,\n",
    "    \"freeze_bert\": True,\n",
    "    \"num_epochs\": 30,\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "run_experiments(param_dict, 6, max_sentences=64, max_seq_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Loss: 0.6983\n",
      "Epoch 2/30 - Loss: 0.6980\n",
      "Epoch 3/30 - Loss: 0.6975\n",
      "Epoch 4/30 - Loss: 0.6973\n",
      "Epoch 5/30 - Loss: 0.6964\n",
      "Epoch 6/30 - Loss: 0.6951\n",
      "Epoch 7/30 - Loss: 0.6943\n",
      "Epoch 8/30 - Loss: 0.6935\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     24\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     26\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Move the inputs to the GPU if available.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# The input tensors have shape: (batch_size, max_sentences, max_seq_length)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[15], line 57\u001b[0m, in \u001b[0;36mTextDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Tokenize each sentence individually.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[0;32m---> 57\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     input_ids_list\u001b[38;5;241m.\u001b[39mappend(encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])       \u001b[38;5;66;03m# Tensor shape: (1, max_seq_length)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     attention_mask_list\u001b[38;5;241m.\u001b[39mappend(encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Tensor shape: (1, max_seq_length)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3207\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3197\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3198\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3199\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3200\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3204\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3205\u001b[0m )\n\u001b[0;32m-> 3207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3210\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3226\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3227\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3228\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:603\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    581\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    601\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    602\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 603\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:541\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    529\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_batch(\n\u001b[1;32m    530\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    531\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    532\u001b[0m     is_pretokenized\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m    533\u001b[0m )\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[0;32m--> 541\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    543\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m    544\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m    545\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m    546\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m    547\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m    548\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m    549\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m    550\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    551\u001b[0m     )\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    553\u001b[0m ]\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# Convert the output to have dict[list] from list[dict] and remove the additional overflows dimension\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# From (variable) shape (batch, overflows, sequence length) to ~ (batch * overflows, sequence length)\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# (we say ~ because the number of overflow varies with the example in the batch)\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# To match each overflowing sample with the original sample in the batch\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# we add an overflow_to_sample_mapping array (see below)\u001b[39;00m\n\u001b[1;32m    561\u001b[0m sanitized_tokens \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:542\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    529\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_batch(\n\u001b[1;32m    530\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    531\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    532\u001b[0m     is_pretokenized\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m    533\u001b[0m )\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    541\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_encoding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    553\u001b[0m ]\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# Convert the output to have dict[list] from list[dict] and remove the additional overflows dimension\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# From (variable) shape (batch, overflows, sequence length) to ~ (batch * overflows, sequence length)\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# (we say ~ because the number of overflow varies with the example in the batch)\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# To match each overflowing sample with the original sample in the batch\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# we add an overflow_to_sample_mapping array (see below)\u001b[39;00m\n\u001b[1;32m    561\u001b[0m sanitized_tokens \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:312\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._convert_encoding\u001b[0;34m(self, encoding, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    310\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m [encoding]\n\u001b[0;32m--> 312\u001b[0m encoding_dict \u001b[38;5;241m=\u001b[39m \u001b[43mdefaultdict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m encodings:\n\u001b[1;32m    314\u001b[0m     encoding_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(e\u001b[38;5;241m.\u001b[39mids)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the model (ensure you have defined BertClassifier accordingly).\n",
    "model = BertClassifier(dropout_rate=0.1, freeze_bert=True, use_attention=True, use_lora= False, max_sentences=128)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Set up training parameters.\n",
    "learning_rate = 2e-5  # For grid-search, iterate over {2e-5, 3e-5, 4e-5, 5e-5}.\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "num_epochs = 30\n",
    "total_steps = len(dataloader) * num_epochs\n",
    "\n",
    "# Scheduler with 100k warm-up steps.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100000, num_training_steps=total_steps)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Early stopping parameters.\n",
    "patience = 3\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Training loop.\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        # Move the inputs to the GPU if available.\n",
    "        # The input tensors have shape: (batch_size, max_sentences, max_seq_length)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass through the model.\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping check.\n",
    "    if avg_loss < best_loss - 5e-4 or epoch <10:\n",
    "        best_loss = avg_loss\n",
    "        epochs_no_improve = 0\n",
    "        # Save the best model checkpoint.\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            with open(\"results.txt\", \"a\") as f:\n",
    "                f.write(\"Early stopping triggered\")\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_908988/2585935597.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5007\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assume df_test is your test DataFrame with the same \"CONCLUSION\" and \"VIOLATED\" columns.\n",
    "df_test = df_dict[\"test\"]\n",
    "test_dataset = TextDataset(df_test, text_column='TEXT', label_column='VIOLATED', max_seq_length=128)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Load the best model checkpoint\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Get predicted labels from softmax probabilities\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "        \n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
