{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import DataLoaderECHR\n",
    "data_path= \"/users/eleves-b/2021/theo.molfessis/LLM/ECHR_Dataset/\"\n",
    "df_dict  = DataLoaderECHR(data_path + \"EN_train\", data_path + \"EN_dev\", data_path + \"EN_test\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from transformers import AutoModel\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 freeze_bert=False, \n",
    "                 use_lora=False, \n",
    "                 lora_rank=4,\n",
    "                 use_attention=False,\n",
    "                 max_sentences=128,\n",
    "                 dropout_rate=0.1,\n",
    "                 modelname   = \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "         - freeze_bert: If True, freeze BERT parameters (unless using LoRA).\n",
    "         - use_lora: If True, apply a LoRA adapter to BERT's linear layers.\n",
    "         - lora_rank: The rank for low-rank adaptation in LoRA.\n",
    "         - use_attention: If True, use a hierarchical attention head over sentence embeddings.\n",
    "                          Otherwise, use an MLP head that concatenates sentence embeddings.\n",
    "         - max_sentences: Fixed number of sentences per document (with padding/truncation).\n",
    "         - dropout_rate: Dropout rate in the classification head.\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Load the pre-trained BERT model.\n",
    "        self.bert = AutoModel.from_pretrained(modelname)\n",
    "        print(\"Loaded BERT model:\", modelname)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        self.use_attention = use_attention\n",
    "        self.max_sentences = max_sentences\n",
    "\n",
    "        # Apply LoRA modifications if requested.\n",
    "        if use_lora:\n",
    "            apply_lora(self.bert, lora_rank)\n",
    "        elif freeze_bert:\n",
    "            # Freeze all BERT parameters if not using LoRA.\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        if use_attention:\n",
    "            # For the hierarchical attention mechanism, we compute explicit query, key, and value.\n",
    "            # The keys and values are computed from each sentence embedding.\n",
    "            self.key_layer = nn.Linear(self.hidden_size, self.hidden_size)    # Projection for keys\n",
    "            self.value_layer = nn.Linear(self.hidden_size, self.hidden_size)  # Projection for values\n",
    "            # A learned context vector (used as the query) that is shared across sentences.\n",
    "            # This is analogous to a fixed query vector in attention mechanisms.\n",
    "            self.context_vector = nn.Parameter(torch.randn(self.hidden_size))\n",
    "            # After attention, we pass the aggregated representation through a dense layer.\n",
    "            self.dense_tanh = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "            self.classifier = nn.Linear(self.hidden_size, 2)\n",
    "        else:\n",
    "            # MLP-based head: concatenate sentence embeddings into a single vector.\n",
    "            self.dense_tanh = nn.Linear(max_sentences * self.hidden_size, max_sentences * self.hidden_size)\n",
    "            self.classifier = nn.Linear(max_sentences * self.hidden_size, 2)\n",
    "            \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for processing a batch of documents.\n",
    "        Each document is represented as a list of sentences.\n",
    "        \n",
    "        Parameters:\n",
    "         - input_ids: Tensor of shape (batch_size, max_sentences, seq_length)\n",
    "         - attention_mask: Tensor of shape (batch_size, max_sentences, seq_length)\n",
    "        \"\"\"\n",
    "        batch_size, num_sentences, seq_length = input_ids.size()\n",
    "        # Flatten the sentences so BERT processes each sentence independently.\n",
    "        input_ids_flat = input_ids.view(-1, seq_length)\n",
    "        attention_mask_flat = attention_mask.view(-1, seq_length)\n",
    "        \n",
    "        # Obtain BERT sentence embeddings (using the pooled [CLS] token representation).\n",
    "        outputs = self.bert(input_ids=input_ids_flat, attention_mask=attention_mask_flat)\n",
    "        pooled_output = outputs.pooler_output if hasattr(outputs, 'pooler_output') else outputs[1]\n",
    "        # Reshape back to (batch_size, num_sentences, hidden_size).\n",
    "        sentence_embeddings = pooled_output.view(batch_size, num_sentences, self.hidden_size)\n",
    "        \n",
    "        if self.use_attention:\n",
    "            # ----------------------\n",
    "            # Hierarchical Attention:\n",
    "            # ----------------------\n",
    "            # 1. Compute Keys (K) from sentence embeddings.\n",
    "            #    Each sentence embedding is projected to obtain its key representation.\n",
    "            K = self.key_layer(sentence_embeddings)  # shape: (batch_size, num_sentences, hidden_size)\n",
    "            \n",
    "            # 2. Compute Values (V) from sentence embeddings.\n",
    "            #    Here, the values are also derived from the sentence embeddings.\n",
    "            V = self.value_layer(sentence_embeddings)  # shape: (batch_size, num_sentences, hidden_size)\n",
    "            \n",
    "            # 3. Use a learned context vector as the Query (Q).\n",
    "            #    Expand the context vector to match the batch size and add a sequence length dimension.\n",
    "            #    Q shape: (batch_size, 1, hidden_size)\n",
    "            Q = self.context_vector.unsqueeze(0).expand(batch_size, -1).unsqueeze(1)\n",
    "            \n",
    "            # 4. Compute scaled dot-product attention scores:\n",
    "            #    scores = (Q * K^T) / sqrt(d_k)\n",
    "            #    Here, K^T means transposing the last two dimensions of K.\n",
    "            scores = torch.matmul(Q, K.transpose(-2, -1))  # shape: (batch_size, 1, num_sentences)\n",
    "            scores = scores / math.sqrt(self.hidden_size)     # scale by sqrt(d_k)\n",
    "            scores = scores.squeeze(1)                         # shape: (batch_size, num_sentences)\n",
    "            \n",
    "            # 5. Create a sentence mask to avoid attending to padded sentences.\n",
    "            #    A sentence is considered valid if its attention_mask has a nonzero sum.\n",
    "            sentence_mask = (attention_mask.sum(dim=2) > 0).float()  # shape: (batch_size, num_sentences)\n",
    "            # Set scores of padded sentences to a very low value.\n",
    "            scores = scores.masked_fill(sentence_mask == 0, -1e9)\n",
    "            \n",
    "            # 6. Apply softmax to obtain attention weights.\n",
    "            attn_weights = torch.softmax(scores, dim=1)          # shape: (batch_size, num_sentences)\n",
    "            attn_weights = attn_weights.unsqueeze(-1)            # shape: (batch_size, num_sentences, 1)\n",
    "            \n",
    "            # 7. Compute the context vector (document embedding) as the weighted sum of the values.\n",
    "            doc_embedding = torch.sum(V * attn_weights, dim=1)     # shape: (batch_size, hidden_size)\n",
    "            \n",
    "            # Pass the aggregated representation through a dense layer.\n",
    "            x = self.dropout(doc_embedding)\n",
    "            x = self.dense_tanh(x)\n",
    "        else:\n",
    "            # ---------------------------\n",
    "            # MLP-based head (non-attention):\n",
    "            # ---------------------------\n",
    "            # Concatenate sentence embeddings into a single vector.\n",
    "            doc_embedding = sentence_embeddings.view(batch_size, -1)  # shape: (batch_size, max_sentences*hidden_size)\n",
    "            x = self.dropout(doc_embedding)\n",
    "            x = self.dense_tanh(x)\n",
    "            \n",
    "        # Apply non-linearity and dropout.\n",
    "        x = self.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        # Final classification layer.\n",
    "        logits = self.classifier(x)\n",
    "        # Convert logits to probabilities.\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "\n",
    "# Helper function for applying LoRA to a module's linear layers.\n",
    "def apply_lora(module, lora_rank):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.Linear):\n",
    "            setattr(module, name, LoRALinear(child, lora_rank))\n",
    "        else:\n",
    "            apply_lora(child, lora_rank)\n",
    "\n",
    "# A simple LoRA wrapper for a linear layer.\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, linear_layer, lora_rank=4, scaling=1.0):\n",
    "        super(LoRALinear, self).__init__()\n",
    "        self.linear = linear_layer\n",
    "        self.lora_rank = lora_rank\n",
    "        self.scaling = scaling\n",
    "        # Create low-rank adaptation matrices.\n",
    "        self.lora_A = nn.Parameter(torch.zeros(lora_rank, linear_layer.in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(linear_layer.out_features, lora_rank))\n",
    "        # Initialize the low-rank matrices.\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "        # Freeze original linear layer parameters.\n",
    "        print(f\"Number of linear parameters: {len(list(self.linear.parameters()))}\")\n",
    "        for param in self.linear.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        original = self.linear(x)\n",
    "        lora_update = self.scaling * ((x @ self.lora_A.t()) @ self.lora_B.t())\n",
    "        return original + lora_update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, text_column, label_column, max_sentences=10, max_seq_length=128,modelname=\"google/bert_uncased_L-2_H-128_A-2\"):\n",
    "        \"\"\"\n",
    "        Dataset that handles the TEXT feature where each entry is a list of sentences.\n",
    "        \n",
    "        Parameters:\n",
    "        - dataframe: Pandas DataFrame containing your data.\n",
    "        - text_column: Name of the column with the TEXT feature (a list of strings).\n",
    "        - label_column: Name of the target label column (binary label: 0 or 1).\n",
    "        - max_sentences: Maximum number of sentences per document (pad or truncate if needed).\n",
    "        - max_seq_length: Maximum sequence length (in tokens) per sentence.\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.text_column = text_column\n",
    "        self.label_column = label_column\n",
    "        self.max_sentences = max_sentences\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the list of sentences for this example.\n",
    "        texts = self.dataframe.iloc[idx][self.text_column]\n",
    "        label = self.dataframe.iloc[idx][self.label_column]\n",
    "        \n",
    "        # Ensure the input is a list. If not, wrap it into a list.\n",
    "        if not isinstance(texts, list):\n",
    "            texts = [texts]\n",
    "        \n",
    "        # Pad or truncate the list of sentences to max_sentences.\n",
    "        if len(texts) < self.max_sentences:\n",
    "            texts = texts + [\"\"] * (self.max_sentences - len(texts))\n",
    "        else:\n",
    "            texts = texts[:self.max_sentences]\n",
    "        \n",
    "        input_ids_list = []\n",
    "        attention_mask_list = []\n",
    "        \n",
    "        # Tokenize each sentence individually.\n",
    "        for sentence in texts:\n",
    "            encoding = self.tokenizer.encode_plus(\n",
    "                sentence,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_seq_length,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            input_ids_list.append(encoding['input_ids'])       # Tensor shape: (1, max_seq_length)\n",
    "            attention_mask_list.append(encoding['attention_mask'])  # Tensor shape: (1, max_seq_length)\n",
    "            \n",
    "        # Concatenate tokenized sentences to form a tensor of shape (max_sentences, max_seq_length)\n",
    "        input_ids = torch.cat(input_ids_list, dim=0)\n",
    "        attention_mask = torch.cat(attention_mask_list, dim=0)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,           # Shape: (max_sentences, max_seq_length)\n",
    "            'attention_mask': attention_mask, # Shape: (max_sentences, max_seq_length)\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the training and development data from your dictionary\n",
    "df_train = df_dict[\"train\"]\n",
    "\n",
    "df_train_dev = df_dict[\"train_dev\"]\n",
    "\n",
    "\n",
    "def run_experiments(param_dict, idxres,max_sentences=64,max_seq_length = 256): \n",
    "    # Create a file to store the results of the experiments.\n",
    "    with open(f\"results_{idxres}.txt\", \"w\") as f:\n",
    "        f.write(f\"modelname: {param_dict['modelname']}, use_attention: {param_dict['use_attention']}, use_lora: {param_dict['use_lora']}, freeze_bert: {param_dict['freeze_bert']}, num_epochs: {param_dict['num_epochs']}, batch_size: {param_dict['batch_size']}\\n\")\n",
    "        f.write(\"Evolution of loss during training\\n\")\n",
    "    \n",
    "    modelname = param_dict[\"modelname\"]\n",
    "    print(f\"Start experiment with model: {modelname}, use_attention: {param_dict['use_attention']}, use_lora: {param_dict['use_lora']}, freeze_bert: {param_dict['freeze_bert']}, num_epochs: {param_dict['num_epochs']}, batch_size: {param_dict['batch_size']}\")\n",
    "    # Initialize tokenizer for the BERT model.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
    "\n",
    "\n",
    "\n",
    "    # Create dataset and dataloader using the TEXT feature instead of CONCLUSION.\n",
    "    print(\"Creating dataset\")\n",
    "    dataset = TextDataset(\n",
    "        df_train_dev,\n",
    "        text_column='TEXT',\n",
    "        label_column='VIOLATED',\n",
    "        max_sentences=max_sentences,\n",
    "        max_seq_length=max_seq_length,\n",
    "        modelname=modelname\n",
    "    )\n",
    "    batch_size = param_dict[\"batch_size\"]\n",
    "    print(\"Loading dataset\")\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    use_attention = param_dict[\"use_attention\"]\n",
    "    use_lora = param_dict[\"use_lora\"]\n",
    "    lora_rank = param_dict[\"lora_rank\"]\n",
    "    freeze_bert = param_dict[\"freeze_bert\"]\n",
    "\n",
    "    # Initialize the model (ensure you have defined BertClassifier accordingly).\n",
    "    model = BertClassifier(dropout_rate=0.1, freeze_bert=freeze_bert, use_attention=use_attention, use_lora= use_lora, modelname=modelname,lora_rank=lora_rank, max_sentences=max_sentences)\n",
    "    print(\"Model initialized\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"Device:\", device)\n",
    "    model = model.to(device)\n",
    "    print(\"Model moved to device\")\n",
    "\n",
    "    # Set up training parameters.\n",
    "    learning_rate = 2e-5  # For grid-search, iterate over {2e-5, 3e-5, 4e-5, 5e-5}.\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    num_epochs = param_dict[\"num_epochs\"]\n",
    "    total_steps = len(dataloader) * num_epochs\n",
    "\n",
    "    # Scheduler with 100k warm-up steps.\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100000, num_training_steps=total_steps)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Early stopping parameters.\n",
    "    patience = 3\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    print(\"Start training\")\n",
    "    # Training loop.\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            # Move the inputs to the GPU if available.\n",
    "            # The input tensors have shape: (batch_size, max_sentences, max_seq_length)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass through the model.\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
    "        with open(f\"results_{idxres}.txt\", \"a\") as f:\n",
    "            f.write(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\\n\")\n",
    "        \n",
    "        # Early stopping check.\n",
    "        if avg_loss < best_loss - 5e-4 or epoch <10:\n",
    "            best_loss = avg_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save the best model checkpoint.\n",
    "            torch.save(model.state_dict(), f\"best_model_{idxres}.pt\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                with open(\"results.txt\", \"a\") as f:\n",
    "                    f.write(\"Early stopping triggered\")\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    def evaluate_only(param_dict, idxres):\n",
    "        modelname = param_dict[\"modelname\"]\n",
    "        model = BertClassifier(dropout_rate=0.1, freeze_bert=param_dict[\"freeze_bert\"], use_attention=param_dict[\"use_attention\"], use_lora= param_dict[\"use_lora\"], modelname=modelname,lora_rank=param_dict[\"lora_rank\"], max_sentences=max_sentences)\n",
    "        # Evaluation\n",
    "        # Assume df_test is your test DataFrame with the same \"CONCLUSION\" and \"VIOLATED\" columns.\n",
    "        df_test = df_dict[\"test\"]\n",
    "        test_dataset = TextDataset(df_test, text_column='TEXT', label_column='VIOLATED',modelname=modelname, max_sentences=max_sentences, max_seq_length=max_seq_length)\n",
    "        test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "        # Load the best model checkpoint\n",
    "\n",
    "        model.load_state_dict(torch.load(f\"best_model_{idxres}.pt\"))\n",
    "        model.eval()\n",
    "\n",
    "        correct_predictions = 0\n",
    "        total_predictions = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_dataloader:\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "                # Get predicted labels from softmax probabilities\n",
    "                _, preds = torch.max(outputs, dim=1)\n",
    "                correct_predictions += (preds == labels).sum().item()\n",
    "                total_predictions += labels.size(0)\n",
    "                \n",
    "        accuracy = correct_predictions / total_predictions\n",
    "        with open(f\"results_{idxres}.txt\", \"a\") as f:\n",
    "            f.write(f\"Test Accuracy: {accuracy:.4f}\\n\")\n",
    "        print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "    evaluate_only(param_dict, idxres)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start experiment with model: nlpaueb/legal-bert-small-uncased, use_attention: False, use_lora: False, freeze_bert: False, num_epochs: 30, batch_size: 16\n",
      "Creating dataset\n",
      "Loading dataset\n",
      "Loaded BERT model: nlpaueb/legal-bert-small-uncased\n",
      "Model initialized\n",
      "Device: cuda\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 23.55 GiB of which 297.25 MiB is free. Including non-PyTorch memory, this process has 23.12 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 16.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m param_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnlpaueb/legal-bert-small-uncased\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m      9\u001b[0m }\n\u001b[0;32m---> 10\u001b[0m \u001b[43mrun_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlegalbert-full-mlp\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[4], line 44\u001b[0m, in \u001b[0;36mrun_experiments\u001b[0;34m(param_dict, idxres, max_sentences, max_seq_length)\u001b[0m\n\u001b[1;32m     42\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     43\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice:\u001b[39m\u001b[38;5;124m\"\u001b[39m, device)\n\u001b[0;32m---> 44\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel moved to device\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     47\u001b[0m \u001b[38;5;66;03m# Set up training parameters.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 4.00 GiB. GPU 0 has a total capacity of 23.55 GiB of which 297.25 MiB is free. Including non-PyTorch memory, this process has 23.12 GiB memory in use. Of the allocated memory 22.81 GiB is allocated by PyTorch, and 16.99 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "param_dict = {\n",
    "    \"modelname\": \"nlpaueb/legal-bert-small-uncased\",\n",
    "    \"use_attention\": False,\n",
    "    \"use_lora\": False,\n",
    "    \"lora_rank\": 4,\n",
    "    \"freeze_bert\": False,\n",
    "    \"num_epochs\": 30,\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "run_experiments(param_dict, \"legalbert-full-mlp\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
