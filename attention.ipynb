{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import DataLoaderECHR\n",
    "data_path= \"/users/eleves-b/2021/theo.molfessis/LLM/ECHR_Dataset/\"\n",
    "df_dict  = DataLoaderECHR(data_path + \"EN_train\", data_path + \"EN_dev\", data_path + \"EN_test\").load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_dict['train']\n",
    "df_train['n_sentences'] = df_train['TEXT'].apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_train['n_sentences'].hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ITEMID</th>\n",
       "      <th>LANGUAGEISOCODE</th>\n",
       "      <th>RESPONDENT</th>\n",
       "      <th>BRANCH</th>\n",
       "      <th>DATE</th>\n",
       "      <th>DOCNAME</th>\n",
       "      <th>IMPORTANCE</th>\n",
       "      <th>CONCLUSION</th>\n",
       "      <th>JUDGES</th>\n",
       "      <th>TEXT</th>\n",
       "      <th>VIOLATED_ARTICLES</th>\n",
       "      <th>VIOLATED_PARAGRAPHS</th>\n",
       "      <th>VIOLATED_BULLETPOINTS</th>\n",
       "      <th>NON_VIOLATED_ARTICLES</th>\n",
       "      <th>NON_VIOLATED_PARAGRAPHS</th>\n",
       "      <th>NON_VIOLATED_BULLETPOINTS</th>\n",
       "      <th>VIOLATED</th>\n",
       "      <th>n_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001-60310</td>\n",
       "      <td>ENG</td>\n",
       "      <td>TUR</td>\n",
       "      <td>CHAMBER</td>\n",
       "      <td>2002</td>\n",
       "      <td>CASE OF SABUKTEKIN v. TURKEY</td>\n",
       "      <td>1</td>\n",
       "      <td>No violation of Art. 2 concerning the death of...</td>\n",
       "      <td>Matti Pellonpää</td>\n",
       "      <td>[7. On 28 September 1994 the applicant's husba...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[13, 2]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>001-93292</td>\n",
       "      <td>ENG</td>\n",
       "      <td>POL</td>\n",
       "      <td>CHAMBER</td>\n",
       "      <td>2009</td>\n",
       "      <td>CASE OF GRZEGORZ HULEWICZ v. POLAND (No. 2)</td>\n",
       "      <td>4</td>\n",
       "      <td>No violation of Article 5 - Right to liberty a...</td>\n",
       "      <td>David Thór Björgvinsson;Giovanni Bonello;Lech ...</td>\n",
       "      <td>[8. The applicant was born in 1974 and lives i...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>0</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>001-93768</td>\n",
       "      <td>ENG</td>\n",
       "      <td>SVK</td>\n",
       "      <td>CHAMBER</td>\n",
       "      <td>2009</td>\n",
       "      <td>CASE OF DVORACEK AND DVORACKOVA v. SLOVAKIA</td>\n",
       "      <td>3</td>\n",
       "      <td>Remainder inadmissible;Violation of Art. 2 (pr...</td>\n",
       "      <td>David Thór Björgvinsson;Giovanni Bonello;Ján Š...</td>\n",
       "      <td>[5. The first applicant, Mr Ivan Dvořáček, was...</td>\n",
       "      <td>[2, 6]</td>\n",
       "      <td>[6-1]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>001-82483</td>\n",
       "      <td>ENG</td>\n",
       "      <td>TUR</td>\n",
       "      <td>CHAMBER</td>\n",
       "      <td>2007</td>\n",
       "      <td>CASE OF MAHMUT ASLAN v. TURKEY</td>\n",
       "      <td>3</td>\n",
       "      <td>Preliminary objections dismissed (victim, non-...</td>\n",
       "      <td>Nicolas Bratza</td>\n",
       "      <td>[4. The applicant was born in 1959 and lives i...</td>\n",
       "      <td>[13, 6]</td>\n",
       "      <td>[6-1]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>001-61852</td>\n",
       "      <td>ENG</td>\n",
       "      <td>LIE</td>\n",
       "      <td>CHAMBER</td>\n",
       "      <td>2004</td>\n",
       "      <td>CASE OF FROMMELT v. LIECHTENSTEIN</td>\n",
       "      <td>3</td>\n",
       "      <td>Violation of Art. 5-4;Non-pecuniary damage - f...</td>\n",
       "      <td>Georg Ress;Mark Villiger</td>\n",
       "      <td>[6. The applicant was born in 1946., 7. On 14 ...</td>\n",
       "      <td>[5]</td>\n",
       "      <td>[5-4]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ITEMID LANGUAGEISOCODE RESPONDENT   BRANCH  DATE  \\\n",
       "0  001-60310             ENG        TUR  CHAMBER  2002   \n",
       "1  001-93292             ENG        POL  CHAMBER  2009   \n",
       "2  001-93768             ENG        SVK  CHAMBER  2009   \n",
       "3  001-82483             ENG        TUR  CHAMBER  2007   \n",
       "4  001-61852             ENG        LIE  CHAMBER  2004   \n",
       "\n",
       "                                       DOCNAME IMPORTANCE  \\\n",
       "0                 CASE OF SABUKTEKIN v. TURKEY          1   \n",
       "1  CASE OF GRZEGORZ HULEWICZ v. POLAND (No. 2)          4   \n",
       "2  CASE OF DVORACEK AND DVORACKOVA v. SLOVAKIA          3   \n",
       "3               CASE OF MAHMUT ASLAN v. TURKEY          3   \n",
       "4            CASE OF FROMMELT v. LIECHTENSTEIN          3   \n",
       "\n",
       "                                          CONCLUSION  \\\n",
       "0  No violation of Art. 2 concerning the death of...   \n",
       "1  No violation of Article 5 - Right to liberty a...   \n",
       "2  Remainder inadmissible;Violation of Art. 2 (pr...   \n",
       "3  Preliminary objections dismissed (victim, non-...   \n",
       "4  Violation of Art. 5-4;Non-pecuniary damage - f...   \n",
       "\n",
       "                                              JUDGES  \\\n",
       "0                                    Matti Pellonpää   \n",
       "1  David Thór Björgvinsson;Giovanni Bonello;Lech ...   \n",
       "2  David Thór Björgvinsson;Giovanni Bonello;Ján Š...   \n",
       "3                                     Nicolas Bratza   \n",
       "4                           Georg Ress;Mark Villiger   \n",
       "\n",
       "                                                TEXT VIOLATED_ARTICLES  \\\n",
       "0  [7. On 28 September 1994 the applicant's husba...                []   \n",
       "1  [8. The applicant was born in 1974 and lives i...                []   \n",
       "2  [5. The first applicant, Mr Ivan Dvořáček, was...            [2, 6]   \n",
       "3  [4. The applicant was born in 1959 and lives i...           [13, 6]   \n",
       "4  [6. The applicant was born in 1946., 7. On 14 ...               [5]   \n",
       "\n",
       "  VIOLATED_PARAGRAPHS VIOLATED_BULLETPOINTS NON_VIOLATED_ARTICLES  \\\n",
       "0                  []                    []               [13, 2]   \n",
       "1                  []                    []                   [5]   \n",
       "2               [6-1]                    []                    []   \n",
       "3               [6-1]                    []                    []   \n",
       "4               [5-4]                    []                    []   \n",
       "\n",
       "  NON_VIOLATED_PARAGRAPHS NON_VIOLATED_BULLETPOINTS  VIOLATED  n_sentences  \n",
       "0                      []                        []         0           90  \n",
       "1                      []                        []         0           18  \n",
       "2                      []                        []         1           32  \n",
       "3                      []                        []         1            8  \n",
       "4                      []                        []         1           17  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_dict[\"train\"].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/users/eleves-b/2021/theo.molfessis/.local/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from transformers import AutoModel\n",
    "\n",
    "class BertClassifier(nn.Module):\n",
    "    def __init__(self, \n",
    "                 freeze_bert=False, \n",
    "                 use_lora=False, \n",
    "                 lora_rank=4,\n",
    "                 use_attention=False,\n",
    "                 max_sentences=128,\n",
    "                 dropout_rate=0.1,\n",
    "                 modelname   = \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "                 ):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "         - freeze_bert: If True, freeze BERT parameters (unless using LoRA).\n",
    "         - use_lora: If True, apply a LoRA adapter to BERT's linear layers.\n",
    "         - lora_rank: The rank for low-rank adaptation in LoRA.\n",
    "         - use_attention: If True, use a hierarchical attention head over sentence embeddings.\n",
    "                          Otherwise, use an MLP head that concatenates sentence embeddings.\n",
    "         - max_sentences: Fixed number of sentences per document (with padding/truncation).\n",
    "         - dropout_rate: Dropout rate in the classification head.\n",
    "        \"\"\"\n",
    "        super(BertClassifier, self).__init__()\n",
    "        # Load the pre-trained BERT model.\n",
    "        self.bert = AutoModel.from_pretrained(modelname)\n",
    "        print(\"Loaded BERT model:\", modelname)\n",
    "        self.hidden_size = self.bert.config.hidden_size\n",
    "        self.use_attention = use_attention\n",
    "        self.max_sentences = max_sentences\n",
    "\n",
    "        # Apply LoRA modifications if requested.\n",
    "        if use_lora:\n",
    "            apply_lora(self.bert, lora_rank)\n",
    "        elif freeze_bert:\n",
    "            # Freeze all BERT parameters if not using LoRA.\n",
    "            for param in self.bert.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "        if use_attention:\n",
    "            # For the hierarchical attention mechanism, we compute explicit query, key, and value.\n",
    "            # The keys and values are computed from each sentence embedding.\n",
    "            self.key_layer = nn.Linear(self.hidden_size, self.hidden_size)    # Projection for keys\n",
    "            self.value_layer = nn.Linear(self.hidden_size, self.hidden_size)  # Projection for values\n",
    "            # A learned context vector (used as the query) that is shared across sentences.\n",
    "            # This is analogous to a fixed query vector in attention mechanisms.\n",
    "            self.context_vector = nn.Parameter(torch.randn(self.hidden_size))\n",
    "            # After attention, we pass the aggregated representation through a dense layer.\n",
    "            self.dense_tanh = nn.Linear(self.hidden_size, self.hidden_size)\n",
    "            self.classifier = nn.Linear(self.hidden_size, 2)\n",
    "        else:\n",
    "            # MLP-based head: concatenate sentence embeddings into a single vector.\n",
    "            self.dense_tanh = nn.Linear(max_sentences * self.hidden_size, max_sentences * self.hidden_size)\n",
    "            self.classifier = nn.Linear(max_sentences * self.hidden_size, 2)\n",
    "            \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for processing a batch of documents.\n",
    "        Each document is represented as a list of sentences.\n",
    "        \n",
    "        Parameters:\n",
    "         - input_ids: Tensor of shape (batch_size, max_sentences, seq_length)\n",
    "         - attention_mask: Tensor of shape (batch_size, max_sentences, seq_length)\n",
    "        \"\"\"\n",
    "        batch_size, num_sentences, seq_length = input_ids.size()\n",
    "        # Flatten the sentences so BERT processes each sentence independently.\n",
    "        input_ids_flat = input_ids.view(-1, seq_length)\n",
    "        attention_mask_flat = attention_mask.view(-1, seq_length)\n",
    "        \n",
    "        # Obtain BERT sentence embeddings (using the pooled [CLS] token representation).\n",
    "        outputs = self.bert(input_ids=input_ids_flat, attention_mask=attention_mask_flat)\n",
    "        pooled_output = outputs.pooler_output if hasattr(outputs, 'pooler_output') else outputs[1]\n",
    "        # Reshape back to (batch_size, num_sentences, hidden_size).\n",
    "        sentence_embeddings = pooled_output.view(batch_size, num_sentences, self.hidden_size)\n",
    "        \n",
    "        if self.use_attention:\n",
    "            # ----------------------\n",
    "            # Hierarchical Attention:\n",
    "            # ----------------------\n",
    "            # 1. Compute Keys (K) from sentence embeddings.\n",
    "            #    Each sentence embedding is projected to obtain its key representation.\n",
    "            K = self.key_layer(sentence_embeddings)  # shape: (batch_size, num_sentences, hidden_size)\n",
    "            \n",
    "            # 2. Compute Values (V) from sentence embeddings.\n",
    "            #    Here, the values are also derived from the sentence embeddings.\n",
    "            V = self.value_layer(sentence_embeddings)  # shape: (batch_size, num_sentences, hidden_size)\n",
    "            \n",
    "            # 3. Use a learned context vector as the Query (Q).\n",
    "            #    Expand the context vector to match the batch size and add a sequence length dimension.\n",
    "            #    Q shape: (batch_size, 1, hidden_size)\n",
    "            Q = self.context_vector.unsqueeze(0).expand(batch_size, -1).unsqueeze(1)\n",
    "            \n",
    "            # 4. Compute scaled dot-product attention scores:\n",
    "            #    scores = (Q * K^T) / sqrt(d_k)\n",
    "            #    Here, K^T means transposing the last two dimensions of K.\n",
    "            scores = torch.matmul(Q, K.transpose(-2, -1))  # shape: (batch_size, 1, num_sentences)\n",
    "            scores = scores / math.sqrt(self.hidden_size)     # scale by sqrt(d_k)\n",
    "            scores = scores.squeeze(1)                         # shape: (batch_size, num_sentences)\n",
    "            \n",
    "            # 5. Create a sentence mask to avoid attending to padded sentences.\n",
    "            #    A sentence is considered valid if its attention_mask has a nonzero sum.\n",
    "            sentence_mask = (attention_mask.sum(dim=2) > 0).float()  # shape: (batch_size, num_sentences)\n",
    "            # Set scores of padded sentences to a very low value.\n",
    "            scores = scores.masked_fill(sentence_mask == 0, -1e9)\n",
    "            \n",
    "            # 6. Apply softmax to obtain attention weights.\n",
    "            attn_weights = torch.softmax(scores, dim=1)          # shape: (batch_size, num_sentences)\n",
    "            attn_weights = attn_weights.unsqueeze(-1)            # shape: (batch_size, num_sentences, 1)\n",
    "            \n",
    "            # 7. Compute the context vector (document embedding) as the weighted sum of the values.\n",
    "            doc_embedding = torch.sum(V * attn_weights, dim=1)     # shape: (batch_size, hidden_size)\n",
    "            \n",
    "            # Pass the aggregated representation through a dense layer.\n",
    "            x = self.dropout(doc_embedding)\n",
    "            x = self.dense_tanh(x)\n",
    "        else:\n",
    "            # ---------------------------\n",
    "            # MLP-based head (non-attention):\n",
    "            # ---------------------------\n",
    "            # Concatenate sentence embeddings into a single vector.\n",
    "            doc_embedding = sentence_embeddings.view(batch_size, -1)  # shape: (batch_size, max_sentences*hidden_size)\n",
    "            x = self.dropout(doc_embedding)\n",
    "            x = self.dense_tanh(x)\n",
    "            \n",
    "        # Apply non-linearity and dropout.\n",
    "        x = self.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        # Final classification layer.\n",
    "        logits = self.classifier(x)\n",
    "        # Convert logits to probabilities.\n",
    "        probs = self.softmax(logits)\n",
    "        return probs\n",
    "\n",
    "# Helper function for applying LoRA to a module's linear layers.\n",
    "def apply_lora(module, lora_rank):\n",
    "    for name, child in module.named_children():\n",
    "        if isinstance(child, nn.Linear):\n",
    "            setattr(module, name, LoRALinear(child, lora_rank))\n",
    "        else:\n",
    "            apply_lora(child, lora_rank)\n",
    "\n",
    "# A simple LoRA wrapper for a linear layer.\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, linear_layer, lora_rank=4, scaling=1.0):\n",
    "        super(LoRALinear, self).__init__()\n",
    "        self.linear = linear_layer\n",
    "        self.lora_rank = lora_rank\n",
    "        self.scaling = scaling\n",
    "        # Create low-rank adaptation matrices.\n",
    "        self.lora_A = nn.Parameter(torch.zeros(lora_rank, linear_layer.in_features))\n",
    "        self.lora_B = nn.Parameter(torch.zeros(linear_layer.out_features, lora_rank))\n",
    "        # Initialize the low-rank matrices.\n",
    "        nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "        nn.init.zeros_(self.lora_B)\n",
    "        # Freeze original linear layer parameters.\n",
    "        for param in self.linear.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        original = self.linear(x)\n",
    "        lora_update = self.scaling * ((x @ self.lora_A.t()) @ self.lora_B.t())\n",
    "        return original + lora_update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import AutoTokenizer, get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, dataframe, text_column, label_column, max_sentences=128, max_seq_length=256,modelname=\"google/bert_uncased_L-2_H-128_A-2\"):\n",
    "        \"\"\"\n",
    "        Dataset that handles the TEXT feature where each entry is a list of sentences.\n",
    "        \n",
    "        Parameters:\n",
    "        - dataframe: Pandas DataFrame containing your data.\n",
    "        - text_column: Name of the column with the TEXT feature (a list of strings).\n",
    "        - label_column: Name of the target label column (binary label: 0 or 1).\n",
    "        - max_sentences: Maximum number of sentences per document (pad or truncate if needed).\n",
    "        - max_seq_length: Maximum sequence length (in tokens) per sentence.\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        self.text_column = text_column\n",
    "        self.label_column = label_column\n",
    "        self.max_sentences = max_sentences\n",
    "        self.max_seq_length = max_seq_length\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Retrieve the list of sentences for this example.\n",
    "        texts = self.dataframe.iloc[idx][self.text_column]\n",
    "        label = self.dataframe.iloc[idx][self.label_column]\n",
    "        \n",
    "        # Ensure the input is a list. If not, wrap it into a list.\n",
    "        if not isinstance(texts, list):\n",
    "            texts = [texts]\n",
    "        \n",
    "        # Pad or truncate the list of sentences to max_sentences.\n",
    "        if len(texts) < self.max_sentences:\n",
    "            texts = texts + [\"\"] * (self.max_sentences - len(texts))\n",
    "        else:\n",
    "            texts = texts[:self.max_sentences]\n",
    "        \n",
    "        input_ids_list = []\n",
    "        attention_mask_list = []\n",
    "        \n",
    "        # Tokenize each sentence individually.\n",
    "        for sentence in texts:\n",
    "            encoding = self.tokenizer.encode_plus(\n",
    "                sentence,\n",
    "                add_special_tokens=True,\n",
    "                max_length=self.max_seq_length,\n",
    "                truncation=True,\n",
    "                padding='max_length',\n",
    "                return_attention_mask=True,\n",
    "                return_tensors='pt'\n",
    "            )\n",
    "            input_ids_list.append(encoding['input_ids'])       # Tensor shape: (1, max_seq_length)\n",
    "            attention_mask_list.append(encoding['attention_mask'])  # Tensor shape: (1, max_seq_length)\n",
    "            \n",
    "        # Concatenate tokenized sentences to form a tensor of shape (max_sentences, max_seq_length)\n",
    "        input_ids = torch.cat(input_ids_list, dim=0)\n",
    "        attention_mask = torch.cat(attention_mask_list, dim=0)\n",
    "        \n",
    "        return {\n",
    "            'input_ids': input_ids,           # Shape: (max_sentences, max_seq_length)\n",
    "            'attention_mask': attention_mask, # Shape: (max_sentences, max_seq_length)\n",
    "            'labels': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the training and development data from your dictionary\n",
    "df_train = df_dict[\"train\"]\n",
    "\n",
    "df_train_dev = df_dict[\"train_dev\"]\n",
    "\n",
    "\n",
    "def run_experiments(param_dict, idxres) : \n",
    "    # Create a file to store the results of the experiments.\n",
    "    with open(f\"results_{idxres}.txt\", \"w\") as f:\n",
    "        f.write(\"modelname,use_attention,use_lora,freeze_bert,num_epochs,batch_size,accuracy\\n\")\n",
    "        f.write(\"Evolution of loss during training\\n\")\n",
    "    \n",
    "    modelname = param_dict[\"modelname\"]\n",
    "    print(f\"Start experiment with model: {modelname}, use_attention: {param_dict['use_attention']}, use_lora: {param_dict['use_lora']}, freeze_bert: {param_dict['freeze_bert']}, num_epochs: {param_dict['num_epochs']}, batch_size: {param_dict['batch_size']}\")\n",
    "    # Initialize tokenizer for the BERT model.\n",
    "    tokenizer = AutoTokenizer.from_pretrained(modelname)\n",
    "\n",
    "\n",
    "\n",
    "    # Create dataset and dataloader using the TEXT feature instead of CONCLUSION.\n",
    "    print(\"Creating dataset\")\n",
    "    dataset = TextDataset(\n",
    "        df_train_dev,\n",
    "        text_column='TEXT',\n",
    "        label_column='VIOLATED',\n",
    "        max_sentences=128,\n",
    "        modelname=modelname\n",
    "    )\n",
    "    batch_size = param_dict[\"batch_size\"]\n",
    "    print(\"Loading dataset\")\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    use_attention = param_dict[\"use_attention\"]\n",
    "    use_lora = param_dict[\"use_lora\"]\n",
    "    lora_rank = param_dict[\"lora_rank\"]\n",
    "    freeze_bert = param_dict[\"freeze_bert\"]\n",
    "\n",
    "    # Initialize the model (ensure you have defined BertClassifier accordingly).\n",
    "    model = BertClassifier(dropout_rate=0.1, freeze_bert=freeze_bert, use_attention=use_attention, use_lora= use_lora, modelname=modelname,lora_rank=lora_rank)\n",
    "    print(\"Model initialized\")\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(\"Device:\", device)\n",
    "    model = model.to(device)\n",
    "    print(\"Model moved to device\")\n",
    "\n",
    "    # Set up training parameters.\n",
    "    learning_rate = 2e-5  # For grid-search, iterate over {2e-5, 3e-5, 4e-5, 5e-5}.\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    num_epochs = param_dict[\"num_epochs\"]\n",
    "    total_steps = len(dataloader) * num_epochs\n",
    "\n",
    "    # Scheduler with 100k warm-up steps.\n",
    "    scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100000, num_training_steps=total_steps)\n",
    "    loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "    # Early stopping parameters.\n",
    "    patience = 3\n",
    "    best_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    print(\"Start training\")\n",
    "    # Training loop.\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            # Move the inputs to the GPU if available.\n",
    "            # The input tensors have shape: (batch_size, max_sentences, max_seq_length)\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Forward pass through the model.\n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "        \n",
    "        avg_loss = total_loss / len(dataloader)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
    "        with open(f\"results_{idxres}.txt\", \"a\") as f:\n",
    "            f.write(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\\n\")\n",
    "        \n",
    "        # Early stopping check.\n",
    "        if avg_loss < best_loss - 5e-4 or epoch <10:\n",
    "            best_loss = avg_loss\n",
    "            epochs_no_improve = 0\n",
    "            # Save the best model checkpoint.\n",
    "            torch.save(model.state_dict(), f\"best_model_{idxres}.pt\")\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            if epochs_no_improve >= patience:\n",
    "                with open(\"results.txt\", \"a\") as f:\n",
    "                    f.write(\"Early stopping triggered\")\n",
    "                print(\"Early stopping triggered\")\n",
    "                break\n",
    "\n",
    "    \n",
    "\n",
    "    # Evaluation\n",
    "    # Assume df_test is your test DataFrame with the same \"CONCLUSION\" and \"VIOLATED\" columns.\n",
    "    df_test = df_dict[\"test\"]\n",
    "    test_dataset = TextDataset(df_test, text_column='TEXT', label_column='VIOLATED',modelname=modelname)\n",
    "    test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Load the best model checkpoint\n",
    "    model.load_state_dict(torch.load(f\"best_model_{idxres}.pt\"))\n",
    "    model.eval()\n",
    "\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            # Get predicted labels from softmax probabilities\n",
    "            _, preds = torch.max(outputs, dim=1)\n",
    "            correct_predictions += (preds == labels).sum().item()\n",
    "            total_predictions += labels.size(0)\n",
    "            \n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    with open(f\"results_{idxres}.txt\", \"a\") as f:\n",
    "        f.write(f\"Test Accuracy: {accuracy:.4f}\\n\")\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start experiment with model: google/bert_uncased_L-2_H-128_A-2, use_attention: False, use_lora: False, freeze_bert: False, num_epochs: 3, batch_size: 16\n",
      "Creating dataset\n",
      "Loading dataset\n",
      "Loaded BERT model: google/bert_uncased_L-2_H-128_A-2\n",
      "Model initialized\n",
      "Device: cuda\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 15.59 GiB of which 538.56 MiB is free. Including non-PyTorch memory, this process has 14.91 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 713.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m param_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodelname\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgoogle/bert_uncased_L-2_H-128_A-2\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muse_attention\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_size\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m16\u001b[39m\n\u001b[1;32m      9\u001b[0m }\n\u001b[0;32m---> 10\u001b[0m \u001b[43mrun_experiments\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[18], line 43\u001b[0m, in \u001b[0;36mrun_experiments\u001b[0;34m(param_dict, idxres)\u001b[0m\n\u001b[1;32m     41\u001b[0m device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDevice:\u001b[39m\u001b[38;5;124m\"\u001b[39m, device)\n\u001b[0;32m---> 43\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel moved to device\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Set up training parameters.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1340\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1337\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1338\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m-> 1340\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:900\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 900\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    902\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    903\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    904\u001b[0m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    905\u001b[0m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    910\u001b[0m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    911\u001b[0m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:927\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    925\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    926\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 927\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    928\u001b[0m p_should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    930\u001b[0m \u001b[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/nn/modules/module.py:1326\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1320\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m   1321\u001b[0m             device,\n\u001b[1;32m   1322\u001b[0m             dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1323\u001b[0m             non_blocking,\n\u001b[1;32m   1324\u001b[0m             memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format,\n\u001b[1;32m   1325\u001b[0m         )\n\u001b[0;32m-> 1326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1327\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1328\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1329\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1330\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1332\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot copy out of meta tensor; no data!\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1024.00 MiB. GPU 0 has a total capacity of 15.59 GiB of which 538.56 MiB is free. Including non-PyTorch memory, this process has 14.91 GiB memory in use. Of the allocated memory 14.02 GiB is allocated by PyTorch, and 713.80 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "param_dict = {\n",
    "    \"modelname\": \"google/bert_uncased_L-2_H-128_A-2\",\n",
    "    \"use_attention\": False,\n",
    "    \"use_lora\": False,\n",
    "    \"lora_rank\": 4,\n",
    "    \"freeze_bert\": False,\n",
    "    \"num_epochs\": 3,\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "run_experiments(param_dict, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30 - Loss: 0.6983\n",
      "Epoch 2/30 - Loss: 0.6980\n",
      "Epoch 3/30 - Loss: 0.6975\n",
      "Epoch 4/30 - Loss: 0.6973\n",
      "Epoch 5/30 - Loss: 0.6964\n",
      "Epoch 6/30 - Loss: 0.6951\n",
      "Epoch 7/30 - Loss: 0.6943\n",
      "Epoch 8/30 - Loss: 0.6935\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 25\u001b[0m\n\u001b[1;32m     23\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     24\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m dataloader:\n\u001b[1;32m     26\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     27\u001b[0m     \u001b[38;5;66;03m# Move the inputs to the GPU if available.\u001b[39;00m\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;66;03m# The input tensors have shape: (batch_size, max_sentences, max_seq_length)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:701\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    698\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    700\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 701\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    705\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    707\u001b[0m ):\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/dataloader.py:757\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    756\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 757\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    758\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    759\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[15], line 57\u001b[0m, in \u001b[0;36mTextDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;66;03m# Tokenize each sentence individually.\u001b[39;00m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m sentence \u001b[38;5;129;01min\u001b[39;00m texts:\n\u001b[0;32m---> 57\u001b[0m     encoding \u001b[38;5;241m=\u001b[39m \u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     58\u001b[0m \u001b[43m        \u001b[49m\u001b[43msentence\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_seq_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmax_length\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     65\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m     input_ids_list\u001b[38;5;241m.\u001b[39mappend(encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m'\u001b[39m])       \u001b[38;5;66;03m# Tensor shape: (1, max_seq_length)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m     attention_mask_list\u001b[38;5;241m.\u001b[39mappend(encoding[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m])  \u001b[38;5;66;03m# Tensor shape: (1, max_seq_length)\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:3207\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   3197\u001b[0m \u001b[38;5;66;03m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   3198\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   3199\u001b[0m     padding\u001b[38;5;241m=\u001b[39mpadding,\n\u001b[1;32m   3200\u001b[0m     truncation\u001b[38;5;241m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3204\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   3205\u001b[0m )\n\u001b[0;32m-> 3207\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   3208\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3209\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_pair\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_pair\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3210\u001b[0m \u001b[43m    \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3211\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3212\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3213\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3214\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3215\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3217\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3226\u001b[0m \u001b[43m    \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msplit_special_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3227\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   3228\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:603\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens, **kwargs)\u001b[0m\n\u001b[1;32m    579\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_encode_plus\u001b[39m(\n\u001b[1;32m    580\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    581\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    600\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    601\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m BatchEncoding:\n\u001b[1;32m    602\u001b[0m     batched_input \u001b[38;5;241m=\u001b[39m [(text, text_pair)] \u001b[38;5;28;01mif\u001b[39;00m text_pair \u001b[38;5;28;01melse\u001b[39;00m [text]\n\u001b[0;32m--> 603\u001b[0m     batched_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_batch_encode_plus\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    604\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatched_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    605\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_split_into_words\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_split_into_words\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    606\u001b[0m \u001b[43m        \u001b[49m\u001b[43madd_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    607\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    608\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtruncation_strategy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtruncation_strategy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    609\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    610\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstride\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    611\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    612\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpadding_side\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpadding_side\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    613\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    614\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    615\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    616\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    617\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    618\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    619\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    620\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    621\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_special_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msplit_special_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    623\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    625\u001b[0m     \u001b[38;5;66;03m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    626\u001b[0m     \u001b[38;5;66;03m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m return_tensors \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:541\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, padding_side, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, split_special_tokens)\u001b[0m\n\u001b[1;32m    529\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_batch(\n\u001b[1;32m    530\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    531\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    532\u001b[0m     is_pretokenized\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m    533\u001b[0m )\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[0;32m--> 541\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    542\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_convert_encoding(\n\u001b[1;32m    543\u001b[0m         encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[1;32m    544\u001b[0m         return_token_type_ids\u001b[38;5;241m=\u001b[39mreturn_token_type_ids,\n\u001b[1;32m    545\u001b[0m         return_attention_mask\u001b[38;5;241m=\u001b[39mreturn_attention_mask,\n\u001b[1;32m    546\u001b[0m         return_overflowing_tokens\u001b[38;5;241m=\u001b[39mreturn_overflowing_tokens,\n\u001b[1;32m    547\u001b[0m         return_special_tokens_mask\u001b[38;5;241m=\u001b[39mreturn_special_tokens_mask,\n\u001b[1;32m    548\u001b[0m         return_offsets_mapping\u001b[38;5;241m=\u001b[39mreturn_offsets_mapping,\n\u001b[1;32m    549\u001b[0m         return_length\u001b[38;5;241m=\u001b[39mreturn_length,\n\u001b[1;32m    550\u001b[0m         verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    551\u001b[0m     )\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    553\u001b[0m ]\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# Convert the output to have dict[list] from list[dict] and remove the additional overflows dimension\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# From (variable) shape (batch, overflows, sequence length) to ~ (batch * overflows, sequence length)\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# (we say ~ because the number of overflow varies with the example in the batch)\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# To match each overflowing sample with the original sample in the batch\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# we add an overflow_to_sample_mapping array (see below)\u001b[39;00m\n\u001b[1;32m    561\u001b[0m sanitized_tokens \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:542\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    529\u001b[0m encodings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tokenizer\u001b[38;5;241m.\u001b[39mencode_batch(\n\u001b[1;32m    530\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    531\u001b[0m     add_special_tokens\u001b[38;5;241m=\u001b[39madd_special_tokens,\n\u001b[1;32m    532\u001b[0m     is_pretokenized\u001b[38;5;241m=\u001b[39mis_split_into_words,\n\u001b[1;32m    533\u001b[0m )\n\u001b[1;32m    535\u001b[0m \u001b[38;5;66;03m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;66;03m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    538\u001b[0m \u001b[38;5;66;03m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;66;03m#                    ]\u001b[39;00m\n\u001b[1;32m    540\u001b[0m \u001b[38;5;66;03m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    541\u001b[0m tokens_and_encodings \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m--> 542\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_convert_encoding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    543\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    544\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_token_type_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    545\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_overflowing_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    547\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_special_tokens_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    548\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_offsets_mapping\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    549\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    550\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    551\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m encoding \u001b[38;5;129;01min\u001b[39;00m encodings\n\u001b[1;32m    553\u001b[0m ]\n\u001b[1;32m    555\u001b[0m \u001b[38;5;66;03m# Convert the output to have dict[list] from list[dict] and remove the additional overflows dimension\u001b[39;00m\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# From (variable) shape (batch, overflows, sequence length) to ~ (batch * overflows, sequence length)\u001b[39;00m\n\u001b[1;32m    557\u001b[0m \u001b[38;5;66;03m# (we say ~ because the number of overflow varies with the example in the batch)\u001b[39;00m\n\u001b[1;32m    558\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# To match each overflowing sample with the original sample in the batch\u001b[39;00m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;66;03m# we add an overflow_to_sample_mapping array (see below)\u001b[39;00m\n\u001b[1;32m    561\u001b[0m sanitized_tokens \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[0;32m~/.local/lib/python3.9/site-packages/transformers/tokenization_utils_fast.py:312\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._convert_encoding\u001b[0;34m(self, encoding, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    310\u001b[0m     encodings \u001b[38;5;241m=\u001b[39m [encoding]\n\u001b[0;32m--> 312\u001b[0m encoding_dict \u001b[38;5;241m=\u001b[39m \u001b[43mdefaultdict\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    313\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m e \u001b[38;5;129;01min\u001b[39;00m encodings:\n\u001b[1;32m    314\u001b[0m     encoding_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_ids\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mappend(e\u001b[38;5;241m.\u001b[39mids)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize the model (ensure you have defined BertClassifier accordingly).\n",
    "model = BertClassifier(dropout_rate=0.1, freeze_bert=True, use_attention=True, use_lora= False, max_sentences=128)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Set up training parameters.\n",
    "learning_rate = 2e-5  # For grid-search, iterate over {2e-5, 3e-5, 4e-5, 5e-5}.\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "num_epochs = 30\n",
    "total_steps = len(dataloader) * num_epochs\n",
    "\n",
    "# Scheduler with 100k warm-up steps.\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100000, num_training_steps=total_steps)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Early stopping parameters.\n",
    "patience = 3\n",
    "best_loss = float('inf')\n",
    "epochs_no_improve = 0\n",
    "\n",
    "# Training loop.\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        # Move the inputs to the GPU if available.\n",
    "        # The input tensors have shape: (batch_size, max_sentences, max_seq_length)\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        # Forward pass through the model.\n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Loss: {avg_loss:.4f}\")\n",
    "    \n",
    "    # Early stopping check.\n",
    "    if avg_loss < best_loss - 5e-4 or epoch <10:\n",
    "        best_loss = avg_loss\n",
    "        epochs_no_improve = 0\n",
    "        # Save the best model checkpoint.\n",
    "        torch.save(model.state_dict(), \"best_model.pt\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            with open(\"results.txt\", \"a\") as f:\n",
    "                f.write(\"Early stopping triggered\")\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    import torch\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_908988/2585935597.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(\"best_model.pt\"))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.5007\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Assume df_test is your test DataFrame with the same \"CONCLUSION\" and \"VIOLATED\" columns.\n",
    "df_test = df_dict[\"test\"]\n",
    "test_dataset = TextDataset(df_test, text_column='TEXT', label_column='VIOLATED', max_seq_length=128)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Load the best model checkpoint\n",
    "model.load_state_dict(torch.load(\"best_model.pt\"))\n",
    "model.eval()\n",
    "\n",
    "correct_predictions = 0\n",
    "total_predictions = 0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_dataloader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        # Get predicted labels from softmax probabilities\n",
    "        _, preds = torch.max(outputs, dim=1)\n",
    "        correct_predictions += (preds == labels).sum().item()\n",
    "        total_predictions += labels.size(0)\n",
    "        \n",
    "accuracy = correct_predictions / total_predictions\n",
    "print(f\"Test Accuracy: {accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
